{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Daniel Lusk, University of Potsdam\n",
    "\n",
    "Inspired by: Ankit Kariryaa ([github repo](https://github.com/ankitkariryaa/An-unexpectedly-large-count-of-trees-in-the-western-Sahara-and-Sahel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import tensorflow as tf\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from config import UNetTraining\n",
    "from core.dataset_generator import DataGenerator\n",
    "from core.frame_info import FrameInfo\n",
    "from core.split_frames import split_dataset\n",
    "from core.UNet import UNet\n",
    "from core.visualize import display_images\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # ignore annoying warnings\n",
    "\n",
    "SHOW_PLOTS = True\n",
    "TRAIN_MODEL = False\n",
    "EXPLORE_PREDICTIONS = False\n",
    "NOTEBOOK = True\n",
    "\n",
    "if NOTEBOOK:\n",
    "    # Magic commands\n",
    "    %matplotlib inline\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    InteractiveShell.ast_node_interactivity = \"last_expr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the configuration and get the image directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = UNetTraining.Config()\n",
    "im_dirs = glob.glob(os.path.join(config.image_dir, \"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all images (aka frames) into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "for d in tqdm(im_dirs):\n",
    "    rgbi_im = rio.open(glob.glob(os.path.join(d, config.rgbi_dn, \"*.tif\"))[0])\n",
    "    ndvi_im = rio.open(glob.glob(os.path.join(d, config.ndvi_dn, \"*.tif\"))[0])\n",
    "    label_im = rio.open(glob.glob(os.path.join(d, config.label_dn, \"*.tif\"))[0])\n",
    "    weights_im = rio.open(glob.glob(os.path.join(d, config.weights_dn, \"*.tif\"))[0])\n",
    "    \n",
    "    read_rgbi_im = (np.moveaxis(rgbi_im.read(), 0, -1)) / 255  # Scale to 0-1\n",
    "    read_ndvi_im = (np.moveaxis(ndvi_im.read(), 0, -1) + 1) / 2  # Scale to 0-1\n",
    "    read_label_im = np.moveaxis(label_im.read(), 0, -1)\n",
    "    read_weights_im = np.moveaxis(weights_im.read(), 0, -1)\n",
    "    \n",
    "    if config.use_binary_labels:\n",
    "        read_label_im[read_label_im > 0] = 1 # Binarize labels\n",
    "    \n",
    "    comb_im = np.dstack((read_rgbi_im, read_ndvi_im))\n",
    "    f = FrameInfo(comb_im, read_label_im, read_weights_im, d)\n",
    "    frames.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train, validation, and test sets, and initialize generators for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_idx, val_frame_idx, test_frame_idx = split_dataset(\n",
    "    frames,\n",
    "    config.frames_json,\n",
    "    config.patch_dir,\n",
    "    test_override=config.test_override,\n",
    "    val_override=config.val_override,\n",
    ")\n",
    "\n",
    "annotation_channels = config.input_label_channel + config.input_weight_channel\n",
    "\n",
    "# Training data generator\n",
    "# Don't apply augmentation for now until the weighting scheme overwriting is figured out.\n",
    "train_generator = DataGenerator(\n",
    "    config.input_image_channels,\n",
    "    config.patch_size,\n",
    "    train_frame_idx,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    wt_thr=config.weight_threshold,\n",
    "    no_weights=config.no_weights\n",
    ").random_generator(config.BATCH_SIZE)\n",
    "\n",
    "# Validation data generator\n",
    "val_generator = DataGenerator(\n",
    "    config.input_image_channels,\n",
    "    config.patch_size,\n",
    "    val_frame_idx,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    wt_thr=config.weight_threshold,\n",
    "    no_weights=config.no_weights\n",
    ").random_generator(config.BATCH_SIZE)\n",
    "\n",
    "# Testing data generator\n",
    "test_generator = DataGenerator(\n",
    "    config.input_image_channels,\n",
    "    config.patch_size,\n",
    "    test_frame_idx,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    wt_thr=config.weight_threshold,\n",
    "    no_weights=config.no_weights\n",
    ").random_generator(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the images to ensure their labels and weights correspond accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    titles = [\"RGB\", \"NIR\", \"NDVI\", \"Labels\", \"Weights\"]\n",
    "\n",
    "    train_images, real_label = next(train_generator)\n",
    "    display_images(\n",
    "        np.concatenate((train_images, real_label), axis=-1),\n",
    "        config.input_image_channels,\n",
    "        annotation_channels,\n",
    "        titles,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model and compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = UNet([config.BATCH_SIZE, *config.input_shape], config.input_label_channel)\n",
    "model.compile(optimizer=config.optimizer, loss=config.loss, metrics=config.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    loss_history = [\n",
    "        model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=config.MAX_TRAIN_STEPS,\n",
    "            epochs=config.EPOCHS,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=config.VAL_LIMIT,\n",
    "            callbacks=config.callbacks,\n",
    "            workers=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE_PREDICTIONS:\n",
    "    # Load model after training\n",
    "    # If you load a model with different python version, then you may run into a\n",
    "    # problem: https://github.com/keras-team/keras/issues/9595#issue-303471777\n",
    "    \n",
    "    model_fn = \"./saved_models/UNet/20230204-1515_AdaDelta_weightmap_tversky_eroded_no-weights-all1_01234_256.h5\"\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_fn,\n",
    "        custom_objects={\n",
    "            \"tversky\": config.loss,\n",
    "            \"dice_coef\": config.metrics[0],\n",
    "            \"dice_loss\": config.metrics[1],\n",
    "            \"accuracy\": config.metrics[4],\n",
    "            \"specificity\": config.metrics[2],\n",
    "            \"sensitivity\": config.metrics[3],\n",
    "        },\n",
    "        compile=False,\n",
    "    )\n",
    "\n",
    "    # In case you want to use multiple GPU you can uncomment the following lines.\n",
    "    # from tensorflow.python.keras.utils import multi_gpu_model\n",
    "    # model = multi_gpu_model(model, gpus=2, cpu_merge=False)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=config.optimizer,\n",
    "        loss=config.loss,\n",
    "        metrics=config.metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    # Print one batch on the training/test data!\n",
    "\n",
    "    titles = [\"RGB\", \"NDVI\", \"Labels\", \"Weights\", \"Prediction\", \"Predictions/Weights Overlay\"]\n",
    "    for i in range(1):\n",
    "        test_images, real_label = next(test_generator)\n",
    "        # 5 images per row: RGB, label, weight, prediction + weight overlay\n",
    "        prediction = model.predict(test_images, steps=1)\n",
    "        # prediction[prediction > 0.5] = 1\n",
    "        # prediction[prediction <= 0.5] = 0\n",
    "\n",
    "        weights = np.expand_dims(real_label[..., 1].copy(), -1)\n",
    "        labels = np.expand_dims(real_label[..., 0].copy(), -1)\n",
    "        weights[weights > 2] = 10 # boundaries\n",
    "        weights[weights <= 2] = 0 # background\n",
    "        # weights[weights == 1] = 3  # background\n",
    "        # weights[weights == 0] = 1  # labels\n",
    "        prediction[prediction >= 0.5] = 1  # predictions\n",
    "        prediction[prediction < 0.5] =  0  # background\n",
    "        # overlay = weights + (prediction * 15)\n",
    "        overlay = weights + prediction + labels\n",
    "        overlay[labels == 1] = 1\n",
    "        overlay[prediction == 1] = 3\n",
    "        overlay[weights == 10] = 5\n",
    "        overlay[(labels == 0) & (prediction == 0) & (weights == 0)] = np.nan\n",
    "        # overlay[overlay == 1] = np.nan\n",
    "\n",
    "        display_images(\n",
    "            np.concatenate((test_images[..., [0, 1, 2, 4]], real_label, prediction, overlay), axis=-1),\n",
    "            config.input_image_channels,\n",
    "            annotation_channels,\n",
    "            titles=titles\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore how the weights are applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    y_pred = prediction[0]\n",
    "    y_t = tf.expand_dims(real_label[0][..., 0].copy(), -1)\n",
    "    y_weights = tf.expand_dims(real_label[0][..., 1].copy(), -1)\n",
    "    alpha = 0.6\n",
    "    beta = 0.4\n",
    "\n",
    "    #### Override weight values to explore effect on losses\n",
    "    y_weights = y_weights.numpy()\n",
    "    y_weights[y_weights >= 2] = 10\n",
    "    y_weights[y_weights < 2] = 1\n",
    "    y_weights = tf.convert_to_tensor(y_weights)\n",
    "\n",
    "    ones = 1\n",
    "    p0 = y_pred  # Probability that pixels are class i\n",
    "    p1 = ones - y_pred  # Probability that pixels are not class i\n",
    "    g0 = y_t  # Ground truth\n",
    "    g1 = ones - y_t\n",
    "\n",
    "    tp_mat = (y_weights * p0 * g0).numpy()\n",
    "    tp = tf.reduce_sum(y_weights * p0 * g0)\n",
    "    fp_mat = (alpha * y_weights * p0 * g1).numpy()\n",
    "    fp = alpha * tf.reduce_sum(y_weights * p0 * g1)\n",
    "    fn_mat = (beta * y_weights * p1 * g0).numpy()\n",
    "    fn = beta * tf.reduce_sum(y_weights * p1 * g0)\n",
    "\n",
    "    EPSILON = 0.00001\n",
    "    numerator = tp\n",
    "    denominator = tp + fp + fn + EPSILON\n",
    "    score = numerator / denominator\n",
    "\n",
    "    score_mat = tp_mat / (tp_mat + fp_mat + fn_mat + EPSILON)\n",
    "\n",
    "    print(\"TP:\", tp)\n",
    "    print(\"FP:\", fp)\n",
    "    print(\"FN:\", fn)\n",
    "    print(\"Score:\", score)\n",
    "\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(14, 4), dpi=250)\n",
    "    ax0.imshow(y_t)\n",
    "    ax0.set_title(\"Labels\")\n",
    "    ax0.axis(\"off\")\n",
    "\n",
    "    ax1.imshow(y_weights)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(\"Weights\")\n",
    "\n",
    "    ax2.imshow(y_pred)\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.set_title(\"Pred\")\n",
    "\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(14, 4), dpi=250)\n",
    "\n",
    "    ax0.imshow(tp_mat)\n",
    "    ax0.set_title(f\"TP: {tp.numpy():.2f}\")\n",
    "    ax0.axis(\"off\")\n",
    "\n",
    "    ax1.imshow(fp_mat)\n",
    "    ax1.set_title(f\"FP: {fp.numpy():.2f}\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(fn_mat)\n",
    "    ax2.set_title(f\"FN: {fn.numpy():.2f}\")\n",
    "    ax2.axis(\"off\")\n",
    "    # fig.suptitle(f\"Score: {score.numpy():.4f}\", fontsize=20, y=1.1)\n",
    "    plt.show();\n",
    "\n",
    "    _, ax = plt.subplots(1, 1, dpi=250)\n",
    "    im = ax.imshow(1 - score_mat)\n",
    "    plt.colorbar(im, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
